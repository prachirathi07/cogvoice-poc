# MemoTag Voice Analysis PoC ðŸ§ 

## What is this?

This project is an **experimental exploration** into using voice analysis to spot early signs of cognitive decline. The system analyzes speech recordings, looking at both acoustic details (like pauses and pitch) and linguistic patterns (like word choice and sentence structure extracted from transcripts generated by Whisper).

Basic AI techniques (unsupervised machine learning) were then applied to see if these voice features could highlight differences or potential risks based on a small initial dataset (10 samples).

**Important Note:** This is a **Proof-of-Concept** built with very limited data. It's **NOT a medical diagnostic tool**. Please consult healthcare professionals for any health concerns.

## Quick Setup & Run

Hereâ€™s how to get it running:

1.  **Get the Code:** Clone or download the project files.
2.  **Data Folders:** Create `data/Control` and `data/Dementia` folders inside the main project directory.
3.  **Training Audio:** Place the 10 required `.wav` training files (5 Control, 5 Dementia, 16kHz mono, from the DementiaBank subset) into the correct folders.
4.  **Install Dependencies:** Open your terminal in the project folder and run:
    ```bash
    # Requires Python 3.9+ and ffmpeg
    pip install -r requirements.txt
    ```
5.  **Train Models (Run Once):** This step processes the training audio and creates the necessary `models.pkl` file.
    ```bash
    python train_pipeline.py
    ```
6.  **Launch the App:** Start the web interface.
    ```bash
    streamlit run app.py
    ```
   
You can then open the provided URL in your browser and upload an audio file to see the experimental analysis.

## The Catch (Limitations Found)

*   **Tiny Dataset:** Training on only 10 files severely limits the models' ability to learn meaningful patterns or generalize. The consistent 50/100 score often seen is a direct result of this.
*   **Basic Features:** The current feature set is foundational; more complex voice aspects weren't explored yet.
*   **STT Dependency:** Results are influenced by Whisper's transcription accuracy.

## What's Next? (Ideas for Improvement)

From working on this PoC, several areas for improvement became clear:

*   **More Data:** Acquiring a much larger dataset (hundreds of samples) is the top priority.
*   **Better Features:** Exploring more sophisticated acoustic and linguistic features could yield better results.
*   **Advanced Models:** With more data, trying supervised learning or sequence models (like LSTMs/RNNs) makes sense.
*   **Live Recording UI:** Adding a feature to record directly in the app would improve usability.
*   **Clinical Validation:** Collaborating with experts is essential to make this genuinely useful.

